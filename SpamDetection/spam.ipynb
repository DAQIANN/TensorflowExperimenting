{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords:\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    stopwords = set([\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ])\n",
    "    sentence = sentence.lower()\n",
    "    words = sentence.split()\n",
    "    end_sentence = \"\"\n",
    "    for i in words:\n",
    "        if i in stopwords:\n",
    "            continue\n",
    "        end_sentence = end_sentence + i + \" \"\n",
    "    end_sentence = end_sentence[:-1]\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham go until jurong point, crazy.. available only in bugis n great world la e buffet... cine there got amore wat...\n"
     ]
    }
   ],
   "source": [
    "temp_list = []\n",
    "labels = []\n",
    "labels_num = []\n",
    "sentences = []\n",
    "with open('SPAM text message 20170820 - Data.csv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        sentence = remove_stopwords(row[1])\n",
    "        sentences.append(sentence)\n",
    "        labels.append(row[0])\n",
    "        if row[0] != \"ham\":\n",
    "            labels_num.append(0)\n",
    "        else:\n",
    "            labels_num.append(1)\n",
    "\n",
    "print(labels[0], sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING = 'post'\n",
    "OOV_TOKEN = \"<OOV>\"\n",
    "TRAIN_SPLIT = 0.8\n",
    "NUM_WORDS = 1000\n",
    "MAXLEN = 120\n",
    "EMBEDDING_DIM = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords:\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    stopwords = set([\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ])\n",
    "    sentence = sentence.lower()\n",
    "    words = sentence.split()\n",
    "    end_sentence = \"\"\n",
    "    for i in words:\n",
    "        if i in stopwords:\n",
    "            continue\n",
    "        end_sentence = end_sentence + i + \" \"\n",
    "    end_sentence = end_sentence[:-1]\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "def train_val_split(sentences, labels, training_split):\n",
    "    train_size = int(len(sentences) * training_split)\n",
    "    train_set, train_labels = sentences[:train_size], labels_num[:train_size]\n",
    "    val_set, val_labels = sentences[train_size:], labels_num[train_size:]\n",
    "    return train_set, train_labels, val_set, val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4457 sentences for training.\n",
      "\n",
      "There are 4457 labels for training.\n",
      "\n",
      "There are 1115 sentences for validation.\n",
      "\n",
      "There are 1115 labels for validation.\n"
     ]
    }
   ],
   "source": [
    "train_set, train_labels, val_set, val_labels = train_val_split(sentences, labels, TRAIN_SPLIT)\n",
    "\n",
    "print(f\"There are {len(train_set)} sentences for training.\\n\")\n",
    "print(f\"There are {len(train_labels)} labels for training.\\n\")\n",
    "print(f\"There are {len(val_set)} sentences for validation.\\n\")\n",
    "print(f\"There are {len(val_labels)} labels for validation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def fit_tokenizer(train_sentences, num_words, oov_token):\n",
    "    tokenizer = Tokenizer(num_words = num_words, oov_token=oov_token)\n",
    "    tokenizer.fit_on_texts(train_sentences)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary contains 8031 words\n",
      "\n",
      "<OOV> token included in vocabulary\n"
     ]
    }
   ],
   "source": [
    "tokenizer = fit_tokenizer(train_set, NUM_WORDS, OOV_TOKEN)\n",
    "word_index = tokenizer.word_index\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(f\"Vocabulary contains {len(word_index)} words\\n\")\n",
    "print(\"<OOV> token included in vocabulary\" if \"<OOV>\" in word_index else \"<OOV> token NOT included in vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates array of token sequences and pads to same length\n",
    "def seq_and_pad(sentences, tokenizer, padding, maxlen):\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    padded_sequences = pad_sequences(sequences, padding=padding, maxlen=maxlen)\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded training sequences have shape: (4457, 120)\n",
      "\n",
      "Padded validation sequences have shape: (1115, 120)\n"
     ]
    }
   ],
   "source": [
    "train_padded_seq = seq_and_pad(train_set, tokenizer, PADDING, MAXLEN)\n",
    "val_padded_seq = seq_and_pad(val_set, tokenizer, PADDING, MAXLEN)\n",
    "\n",
    "print(f\"Padded training sequences have shape: {train_padded_seq.shape}\\n\")\n",
    "print(f\"Padded validation sequences have shape: {val_padded_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize labels\n",
    "# should not be needed since we can just do ham = 0 and spam = 1\n",
    "# Still should try this and see the result\n",
    "def tokenize_labels(all_labels, split_labels):\n",
    "    label_tokenizer = Tokenizer()\n",
    "    label_tokenizer.fit_on_texts(all_labels)\n",
    "    label_seq = np.array(label_tokenizer.texts_to_sequences(split_labels))\n",
    "    label_seq_np = np.array([i - 1 for i in label_seq]) # Since keras starts values at 1\n",
    "    return label_seq_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_token_labels = tokenize_labels(labels, train_labels)\n",
    "# val_token_labels = tokenize_labels(labels, val_labels)\n",
    "\n",
    "# print(f\"First 5 labels of the training set should look like this:\\n{train_token_labels[:5]}\\n\")\n",
    "# print(f\"First 5 labels of the validation set should look like this:\\n{val_token_labels[:5]}\\n\")\n",
    "# print(f\"Tokenized labels of the training set have shape: {train_token_labels.shape}\\n\")\n",
    "# print(f\"Tokenized labels of the validation set have shape: {val_token_labels.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Model\n",
    "def create_model(num_words, embedding_dim, maxlen):\n",
    "    tf.random.set_seed(123)\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(num_words, embedding_dim, input_length=maxlen),\n",
    "        tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
    "        tf.keras.layers.GlobalMaxPooling1D(),\n",
    "        tf.keras.layers.Dense(6, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 120, 16)           128512    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 116, 128)          10368     \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 128)               0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 774       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 139661 (545.55 KB)\n",
      "Trainable params: 139661 (545.55 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(total_words, EMBEDDING_DIM, MAXLEN)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.0025 - accuracy: 0.9989 - val_loss: 0.0610 - val_accuracy: 0.9857\n",
      "Epoch 2/10\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.0024 - accuracy: 0.9987 - val_loss: 0.0675 - val_accuracy: 0.9874\n",
      "Epoch 3/10\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0021 - accuracy: 0.9991 - val_loss: 0.0683 - val_accuracy: 0.9830\n",
      "Epoch 4/10\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0023 - accuracy: 0.9991 - val_loss: 0.0706 - val_accuracy: 0.9848\n",
      "Epoch 5/10\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 0.9989 - val_loss: 0.0732 - val_accuracy: 0.9839\n",
      "Epoch 6/10\n",
      "140/140 [==============================] - 0s 4ms/step - loss: 0.0017 - accuracy: 0.9991 - val_loss: 0.0777 - val_accuracy: 0.9848\n",
      "Epoch 7/10\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.0019 - accuracy: 0.9989 - val_loss: 0.0846 - val_accuracy: 0.9857\n",
      "Epoch 8/10\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0018 - accuracy: 0.9989 - val_loss: 0.0775 - val_accuracy: 0.9830\n",
      "Epoch 9/10\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.0017 - accuracy: 0.9989 - val_loss: 0.0775 - val_accuracy: 0.9839\n",
      "Epoch 10/10\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.0018 - accuracy: 0.9991 - val_loss: 0.0833 - val_accuracy: 0.9830\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_padded_seq, np.array(train_labels), epochs = 10, validation_data=(val_padded_seq, np.array(val_labels))) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7c31fab77c3cd1e9f3a0cb381c964276c59fad45f57b06737b006c59b7c8782"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
